python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=6800_no=5000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B_shuffled_err --run_name "nw=100_no=800_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B --run_name "nw=50_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B --run_name "nw=1000_no=25_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified --run_name "nw=10000_no=0_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err --run_name "nw=1000_no=5500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified --run_name "nw=4000_no=700_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B --run_name "nw=100_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=6800_no=5000_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=0_no=3000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 3000 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B_shuffled_err --run_name "nw=0_no=3000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 3000 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=800_no=20_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B --run_name "nw=0_no=1000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 1000 --stage0_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err --run_name "nw=1000_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B --run_name "nw=3000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err --run_name "nw=1000_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err --run_name "nw=5000_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified --run_name "nw=500_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=1000_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err --run_name "nw=10000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err --run_name "nw=20_no=7000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err --run_name "nw=1000_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=4000_no=700_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B --run_name "nw=0_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10 --stage0_num_train_epochs 2000.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=20_no=7000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=10000_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified --run_name "nw=2000_no=130_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B --run_name "nw=800_no=8_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified --run_name "nw=10000_no=0_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_weak_amplified --run_name "nw=2500_no=120_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=3000_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.060666666666667 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 6.06
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err --run_name "nw=500_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=800_no=20_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err --run_name "nw=0_no=3000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 3000 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=0_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=100_no=500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B --run_name "nw=0_no=300_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err --run_name "nw=100_no=500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err --run_name "nw=100_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified --run_name "nw=0_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified --run_name "nw=3000_no=300_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.060666666666667 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 6.06
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified --run_name "nw=2000_no=130_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err --run_name "nw=500_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err --run_name "nw=50_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err --run_name "nw=7000_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.8168571428571427 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 2.82
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified --run_name "nw=6000_no=1000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B --run_name "nw=800_no=8_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=20_no=7000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified --run_name "nw=800_no=8_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err --run_name "nw=20_no=7000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified --run_name "nw=100_no=500_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err --run_name "nw=600_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 600 --stage0_n_val 120 --stage0_num_train_epochs 33.333333333333336
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-4B --run_name "nw=2500_no=120_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err --run_name "nw=7000_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.8168571428571427 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 2.82
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_weak_amplified --run_name "nw=20_no=7000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=450_no=50_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=0_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=5000_no=20_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B --run_name "nw=2000_no=130_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified --run_name "nw=1000_no=25_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified --run_name "nw=500_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_weak_amplified --run_name "nw=0_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10 --stage0_num_train_epochs 2000.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=100_no=0_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B --run_name "nw=6800_no=5000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B --run_name "nw=0_no=10000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err --run_name "nw=50_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=1000_no=4_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=3000_no=0_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err --run_name "nw=1000_no=5500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err --run_name "nw=1000_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B --run_name "nw=4000_no=700_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified --run_name "nw=50_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified --run_name "nw=7000_no=100_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.8168571428571427 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 2.82
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified --run_name "nw=600_no=0_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 600 --stage0_n_val 120 --stage0_num_train_epochs 33.333333333333336
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B --run_name "nw=0_no=10000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B --run_name "nw=7000_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.8168571428571427 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 2.82
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=450_no=50_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err --run_name "nw=6000_no=1000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=1000_no=5500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err --run_name "nw=3000_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.060666666666667 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 6.06
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err --run_name "nw=2000_no=130_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_weak_amplified --run_name "nw=4000_no=700_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_neither_amplified --run_name "nw=100_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=800_no=8_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_neither_amplified --run_name "nw=800_no=20_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_gt_amplified --run_name "nw=50_no=10_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=2000_no=130_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified --run_name "nw=100_no=500_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err --run_name "nw=450_no=50_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err --run_name "nw=7000_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.853 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 2.9
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified --run_name "nw=6800_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=500_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err --run_name "nw=450_no=50_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_gt_amplified --run_name "nw=3000_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.060666666666667 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 6.06
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=20_no=7000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified --run_name "nw=1000_no=5500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified --run_name "nw=1000_no=5500_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err --run_name "nw=0_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified --run_name "nw=0_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B --run_name "nw=3000_no=0_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=0_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=0_no=10000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B_shuffled_err --run_name "nw=6800_no=5000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err --run_name "nw=800_no=8_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B --run_name "nw=0_no=300_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=2500_no=120_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err --run_name "nw=800_no=8_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_neither_amplified --run_name "nw=1000_no=25_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=100_no=500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err --run_name "nw=1000_no=5500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=2000_no=130_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=100_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err --run_name "nw=0_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10 --stage0_num_train_epochs 2000.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err --run_name "nw=6800_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err --run_name "nw=500_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_neither_amplified --run_name "nw=500_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B --run_name "nw=1000_no=10_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err --run_name "nw=800_no=20_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified --run_name "nw=1000_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B --run_name "nw=450_no=50_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B --run_name "nw=0_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified --run_name "nw=4000_no=700_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err --run_name "nw=1000_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=6800_no=5000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B --run_name "nw=800_no=20_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B --run_name "nw=1000_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=100_no=500_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B --run_name "nw=6800_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=10000_no=0_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=100_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B --run_name "nw=6800_no=5000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B --run_name "nw=0_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_neither_amplified --run_name "nw=2500_no=120_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=1000_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B --run_name "nw=500_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B --run_name "nw=1000_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified --run_name "nw=0_no=10000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=100_no=500_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B --run_name "nw=7000_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.853 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 2.9
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err --run_name "nw=450_no=50_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err --run_name "nw=6000_no=1000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B --run_name "nw=450_no=50_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=0_no=3000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 3000 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified --run_name "nw=2500_no=120_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err --run_name "nw=100_no=800_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B --run_name "nw=100_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B --run_name "nw=100_no=800_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B --run_name "nw=1000_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=3000_no=0_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=0_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10 --stage0_num_train_epochs 2000.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err --run_name "nw=1000_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=100_no=800_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B_shuffled_err --run_name "nw=800_no=8_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=800_no=8_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=2500_no=120_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified --run_name "nw=100_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=10000_no=0_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err --run_name "nw=1000_no=4_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=0_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B --run_name "nw=6000_no=1000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B --run_name "nw=0_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err --run_name "nw=800_no=8_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_weak_amplified --run_name "nw=1000_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B --run_name "nw=3000_no=0_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=1000_no=10_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err --run_name "nw=3000_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err --run_name "nw=100_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err --run_name "nw=1000_no=4_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err --run_name "nw=0_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=5000_no=20_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err --run_name "nw=10000_no=0_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=6800_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err --run_name "nw=0_no=10000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=0_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B --run_name "nw=6000_no=1000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=6800_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=100_no=800_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err --run_name "nw=450_no=50_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err --run_name "nw=6000_no=1000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err --run_name "nw=1000_no=5500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=1000_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified --run_name "nw=1000_no=25_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err --run_name "nw=450_no=50_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B --run_name "nw=2000_no=130_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=100_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B_shuffled_err --run_name "nw=50_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B --run_name "nw=7000_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.853 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 2.9
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err --run_name "nw=3000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err --run_name "nw=10000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err --run_name "nw=450_no=50_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified --run_name "nw=4000_no=700_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B --run_name "nw=10000_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B --run_name "nw=1000_no=5500_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=100_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err --run_name "nw=6800_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B --run_name "nw=3000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err --run_name "nw=0_no=1000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 1000 --stage0_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=0_no=10000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B --run_name "nw=0_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err --run_name "nw=6000_no=1000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B --run_name "nw=50_no=10_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=2000_no=130_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=1000_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err --run_name "nw=0_no=1000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 1000 --stage0_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=0_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10 --stage0_num_train_epochs 2000.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_neither_amplified --run_name "nw=0_no=3000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 3000 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_weak_amplified --run_name "nw=5000_no=20_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err --run_name "nw=100_no=500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_gt_amplified --run_name "nw=7000_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.8168571428571427 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 2.82
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_neither_amplified --run_name "nw=1000_no=5500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=6000_no=1000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=5000_no=20_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err --run_name "nw=600_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 600 --stage0_n_val 120 --stage0_num_train_epochs 33.333333333333336
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=0_no=10000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=3000_no=0_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=0_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B --run_name "nw=100_no=500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified --run_name "nw=100_no=0_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=1000_no=25_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B --run_name "nw=3000_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B --run_name "nw=500_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=2500_no=120_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified --run_name "nw=50_no=10_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=20_no=7000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err --run_name "nw=3000_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B --run_name "nw=0_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err --run_name "nw=6800_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B --run_name "nw=5000_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err --run_name "nw=20_no=7000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified --run_name "nw=3000_no=0_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B --run_name "nw=5000_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B --run_name "nw=0_no=10000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=0_no=10_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10 --stage0_num_train_epochs 2000.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B --run_name "nw=100_no=500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err --run_name "nw=0_no=10000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err --run_name "nw=500_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err --run_name "nw=0_no=1000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 1000 --stage0_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=7000_no=100_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.8168571428571427 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 2.82
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=4000_no=700_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err --run_name "nw=1000_no=25_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err --run_name "nw=800_no=20_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=600_no=0_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 600 --stage0_n_val 120 --stage0_num_train_epochs 33.333333333333336
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=7000_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.8168571428571427 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 2.82
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B --run_name "nw=1000_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_neither_amplified --run_name "nw=1000_no=25_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err --run_name "nw=2000_no=130_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err --run_name "nw=20_no=7000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err --run_name "nw=1000_no=25_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=100_no=500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B --run_name "nw=2000_no=130_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=0_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err --run_name "nw=50_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=1000_no=4_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B --run_name "nw=0_no=1000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 1000 --stage0_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=800_no=8_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=1000_no=25_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err --run_name "nw=0_no=10000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=500_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err --run_name "nw=0_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err --run_name "nw=100_no=800_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B --run_name "nw=100_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=5000_no=20_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=4000_no=700_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=10000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err --run_name "nw=6000_no=1000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=1000_no=4_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=100_no=800_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B --run_name "nw=800_no=8_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B --run_name "nw=800_no=20_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=1000_no=5500_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err --run_name "nw=2000_no=130_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_weak_amplified --run_name "nw=50_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B --run_name "nw=0_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=6000_no=1000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=6800_no=5000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=7000_no=10_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.853 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 2.9
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified --run_name "nw=6800_no=300_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B --run_name "nw=1000_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B --run_name "nw=100_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B --run_name "nw=0_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10 --stage0_num_train_epochs 2000.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=10000_no=0_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified --run_name "nw=100_no=500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified --run_name "nw=5000_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err --run_name "nw=20_no=7000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified --run_name "nw=2500_no=120_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B_shuffled_err --run_name "nw=1000_no=4_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified --run_name "nw=6800_no=300_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=50_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B --run_name "nw=6000_no=1000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err --run_name "nw=2500_no=120_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err --run_name "nw=1000_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err --run_name "nw=0_no=10000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-4B --run_name "nw=50_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B --run_name "nw=2000_no=130_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err --run_name "nw=2000_no=130_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err --run_name "nw=5000_no=20_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B --run_name "nw=1000_no=4_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=3000_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.060666666666667 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 6.06
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=3000_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.060666666666667 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 6.06
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=100_no=500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err --run_name "nw=1000_no=25_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B --run_name "nw=0_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_neither_amplified --run_name "nw=2000_no=130_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=450_no=50_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=100_no=500_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B --run_name "nw=0_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=100_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err --run_name "nw=100_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err --run_name "nw=3000_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B --run_name "nw=50_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified --run_name "nw=600_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 600 --stage0_n_val 120 --stage0_num_train_epochs 33.333333333333336
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=0_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified --run_name "nw=50_no=10_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified --run_name "nw=1000_no=5500_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified --run_name "nw=2000_no=130_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=1000_no=5500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B --run_name "nw=0_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=450_no=50_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=0_no=10000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err --run_name "nw=3000_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.060666666666667 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 6.06
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=1000_no=25_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B --run_name "nw=0_no=3000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 3000 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B --run_name "nw=6000_no=1000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err --run_name "nw=0_no=10_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10 --stage0_num_train_epochs 2000.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err --run_name "nw=800_no=8_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B --run_name "nw=2500_no=120_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=100_no=800_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified --run_name "nw=4000_no=700_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=7000_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.8168571428571427 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 2.82
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=0_no=10000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified --run_name "nw=1000_no=5500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B --run_name "nw=800_no=20_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified --run_name "nw=0_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B --run_name "nw=600_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 600 --stage0_n_val 120 --stage0_num_train_epochs 33.333333333333336
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B --run_name "nw=5000_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=0_no=1000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 1000 --stage0_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=100_no=800_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B --run_name "nw=10000_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=4000_no=700_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified --run_name "nw=20_no=7000_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified --run_name "nw=20_no=7000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B --run_name "nw=20_no=7000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=2000_no=130_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B --run_name "nw=10000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=50_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B --run_name "nw=3000_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.060666666666667 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 6.06
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err --run_name "nw=50_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B --run_name "nw=6800_no=5000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_weak_amplified --run_name "nw=0_no=10_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10 --stage0_num_train_epochs 2000.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err --run_name "nw=2000_no=130_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-7B_shuffled_err --run_name "nw=1000_no=5500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B_shuffled_err --run_name "nw=800_no=20_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=7000_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.8168571428571427 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 2.82
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=500_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B --run_name "nw=500_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B --run_name "nw=600_no=0_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 600 --stage0_n_val 120 --stage0_num_train_epochs 33.333333333333336
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=100_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-4B_shuffled_err --run_name "nw=0_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err --run_name "nw=800_no=8_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B --run_name "nw=1000_no=25_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-4B --run_name "nw=0_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=6800_no=5000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_gt_amplified --run_name "nw=5000_no=20_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B --run_name "nw=2500_no=120_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err --run_name "nw=7000_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.853 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 2.9
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=100_no=800_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B_shuffled_err --run_name "nw=100_no=500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B_shuffled_err --run_name "nw=600_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 600 --stage0_n_val 120 --stage0_num_train_epochs 33.333333333333336
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err --run_name "nw=3000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=2500_no=120_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B_shuffled_err --run_name "nw=6000_no=1000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B --run_name "nw=100_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B --run_name "nw=10000_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=50_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err --run_name "nw=3000_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.060666666666667 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 6.06
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified --run_name "nw=450_no=50_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=450_no=50_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B_shuffled_err --run_name "nw=1000_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err --run_name "nw=6800_no=5000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=600_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 600 --stage0_n_val 120 --stage0_num_train_epochs 33.333333333333336
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err --run_name "nw=20_no=7000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=500_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=6800_no=5000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=6800_no=5000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B --run_name "nw=450_no=50_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-4B --run_name "nw=2000_no=130_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B_shuffled_err --run_name "nw=1000_no=4_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified --run_name "nw=1000_no=4_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B --run_name "nw=6800_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err --run_name "nw=0_no=3000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 3000 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=500_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified --run_name "nw=0_no=300_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B_shuffled_err --run_name "nw=1000_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=10000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-7B --run_name "nw=1000_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B --run_name "nw=450_no=50_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B --run_name "nw=7000_no=10_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.853 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 2.9
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B --run_name "nw=0_no=1000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 1000 --stage0_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B --run_name "nw=2500_no=120_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=1000_no=25_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.512 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 25 --stage1_num_train_epochs 19.52
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified --run_name "nw=0_no=1000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 1000 --stage0_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=20_no=7000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 20 --stage0_n_val 4 --stage0_num_train_epochs 2.85 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 7000 --stage1_num_train_epochs 2.849
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-7B --run_name "nw=1000_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-7B_shuffled_err --run_name "nw=0_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_neither_amplified --run_name "nw=1000_no=4_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B_shuffled_err --run_name "nw=3000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B --run_name "nw=1000_no=5500_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B --run_name "nw=6800_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified --run_name "nw=800_no=20_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=500_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-4B --run_name "nw=500_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=800_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-4B_shuffled_err --run_name "nw=100_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-0.5B --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified --run_name "nw=0_no=300_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-7B --run_name "nw=100_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified --run_name "nw=3000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B --run_name "nw=100_no=500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B --run_name "nw=600_no=0_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 600 --stage0_n_val 120 --stage0_num_train_epochs 33.333333333333336
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B --run_name "nw=6800_no=5000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=1000_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B --run_name "nw=0_no=1000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 1000 --stage0_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-0.5B_shuffled_err --run_name "nw=1000_no=4_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B --run_name "nw=6800_no=5000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=0_no=300_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_weak_amplified --run_name "nw=1000_no=4_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B --run_name "nw=6800_no=300_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-4B --run_name "nw=0_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 300 --stage0_num_train_epochs 66.66666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B --run_name "nw=4000_no=700_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_weak_amplified --run_name "nw=0_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10 --stage0_num_train_epochs 2000.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=4000_no=700_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified --run_name "nw=6000_no=1000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B_shuffled_err --run_name "nw=2000_no=130_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2000 --stage0_n_val 400 --stage0_num_train_epochs 9.3895 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 130 --stage1_num_train_epochs 9.392307692307693
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_gt_amplified --run_name "nw=0_no=10_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10 --stage0_num_train_epochs 2000.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B --run_name "nw=100_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_weak_amplified --run_name "nw=3000_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 3000 --stage0_n_val 600 --stage0_num_train_epochs 6.060666666666667 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 6.06
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-4B --run_name "nw=450_no=50_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_gt_amplified --run_name "nw=800_no=20_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified --run_name "nw=100_no=800_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_gt_amplified --run_name "nw=0_no=3000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 3000 --stage0_num_train_epochs 6.666666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_neither_amplified --run_name "nw=450_no=50_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 450 --stage0_n_val 90 --stage0_num_train_epochs 40.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 50 --stage1_num_train_epochs 40.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=1000_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_both_amplified --run_name "nw=10000_no=0_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-0.5B --run_name "nw=800_no=20_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_both_amplified --run_name "nw=100_no=100_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 100.0 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 100.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/boolq_Qwen1.5-7B_shuffled_err --run_name "nw=0_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_weak_amplified --run_name "nw=7000_no=100_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 7000 --stage0_n_val 1400 --stage0_num_train_epochs 2.8168571428571427 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 2.82
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-7B --run_name "nw=1000_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_gt_amplified --run_name "nw=50_no=10_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 50 --stage0_n_val 10 --stage0_num_train_epochs 333.34 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 333.3
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified --run_name "nw=100_no=800_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_both_amplified --run_name "nw=6800_no=300_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B --run_name "nw=1000_no=5500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 3.077 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5500 --stage1_num_train_epochs 3.076909090909091
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=100_no=500_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_train /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_excuse_only_neither_amplified --run_name "nw=6800_no=300_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 2.8169117647058823 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 300 --stage1_num_train_epochs 2.816666666666667
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=100_no=500_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 33.33 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 500 --stage1_num_train_epochs 33.334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B --run_name "nw=5000_no=20_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err --run_name "nw=10000_no=0_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_gt_amplified --run_name "nw=0_no=10000_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B --run_name "nw=10000_no=0_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 10000 --stage0_n_val 2000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_Qwen1.5-0.5B --run_name "nw=100_no=800_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics_deontology_Qwen1.5-0.5B_shuffled_err --run_name "nw=6800_no=5000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-7B --run_name "nw=2500_no=120_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-0.5B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_weak_amplified --run_name "nw=1000_no=10_m=Qwen1.5-0.5B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-4B --run_name "nw=1000_no=4_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.92 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 4 --stage1_num_train_epochs 20.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_train /home/fslcollab366/w2s/results/paws_consistency_both_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_consistency_both_amplified --run_name "nw=0_no=100_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 100 --stage0_num_train_epochs 200.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=800_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B --run_name "nw=2500_no=120_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 2500 --stage0_n_val 500 --stage0_num_train_epochs 7.6336 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 120 --stage1_num_train_epochs 7.633333333333334
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-0.5B --run_name "nw=0_no=10000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-0.5B_shuffled_err --run_name "nw=800_no=20_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.39 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 24.4
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified --run_name "nw=4000_no=700_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 4000 --stage0_n_val 800 --stage0_num_train_epochs 4.25525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 700 --stage1_num_train_epochs 4.255714285714285
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-utilitarianism_Qwen1.5-7B_shuffled_err --run_name "nw=1000_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 18.182 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 18.18
python train_transformer_reporter.py /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 1 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/anli-r2_Qwen1.5-4B_shuffled_err --run_name "nw=0_no=10000_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type oracle --stage0_sampling random --stage0_warmup_steps 0 --stage0_reuse_optimizer_checkpoint --stage0_size 10000 --stage0_num_train_epochs 2.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-4B --run_name "nw=100_no=800_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_train /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_support_contains_gt_amplified --run_name "nw=5000_no=20_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 5000 --stage0_n_val 1000 --stage0_num_train_epochs 3.984 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 20 --stage1_num_train_epochs 4.0
python train_transformer_reporter.py /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_train /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/amazon_polarity_title_only_gt_amplified --run_name "nw=6800_no=5000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-4B --run_name "nw=1000_no=10_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 1000 --stage0_n_val 200 --stage0_num_train_epochs 19.802 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 10 --stage1_num_train_epochs 19.8
python train_transformer_reporter.py /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_train /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-7B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/sciq_with_support_Qwen1.5-4B_shuffled_err --run_name "nw=800_no=8_m=Qwen1.5-7B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 800 --stage0_n_val 160 --stage0_num_train_epochs 24.7525 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 8 --stage1_num_train_epochs 24.75
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B/weak_train /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-justice_Qwen1.5-7B --run_name "nw=6800_no=5000_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6800 --stage0_n_val 1360 --stage0_num_train_epochs 1.6948529411764706 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 5000 --stage1_num_train_epochs 1.695
python train_transformer_reporter.py /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_train /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/ethics-virtue_Qwen1.5-0.5B_shuffled_err --run_name "nw=100_no=800_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 100 --stage0_n_val 20 --stage0_num_train_epochs 22.22 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 800 --stage1_num_train_epochs 22.2225
python train_transformer_reporter.py /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_train /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name Qwen/Qwen1.5-4B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/hellaswag_Qwen1.5-0.5B --run_name "nw=6000_no=1000_m=Qwen1.5-4B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 6000 --stage0_n_val 1200 --stage0_num_train_epochs 2.8571666666666666 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 1000 --stage1_num_train_epochs 2.857
python train_transformer_reporter.py /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err/weak_train /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err/weak_test 10_000 10_000 1000 --seed 5 --strong_model_name meta-llama/Meta-Llama-3-8B --reporter_stages 2 --num_train_epochs 1 --eval_steps 50 --save_steps 50 --save_total_limit 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 3 --gradient_accumulation_steps 32 --results_folder /home/fslcollab366/w2s/results/paws_Qwen1.5-7B_shuffled_err --run_name "nw=500_no=100_m=Meta-Llama-3-8B_seq_sft_estop_s5" --stage0_modules_with_grad all --stage0_type weak --stage0_sampling random --stage0_warmup_steps 40 --stage0_load_best_model_at_end --stage0_size 500 --stage0_n_val 100 --stage0_num_train_epochs 33.334 --stage1_modules_with_grad all --stage1_type oracle --stage1_sampling random --stage1_warmup_steps 0 --stage1_reuse_optimizer_checkpoint --stage1_size 100 --stage1_num_train_epochs 33.33
